{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = 'XXX'\n",
    "MODEL = 'neuralmind/bert-base-portuguese-cased' #'raquelsilveira/legalbertpt_fp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omutAK9ZcPPb"
   },
   "source": [
    "## Instalando e importando libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1699290447753,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "c5lqYjOlD9RK"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKHk1dlLcabs"
   },
   "source": [
    "## Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(PATH_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6BFz7wXc-nl"
   },
   "source": [
    "## Divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699290453017,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "s_oKENqApAi5"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split_dataset(df_data, doc_name):\n",
    "\n",
    "    df = df_data.copy()\n",
    "\n",
    "    df_label = pd.DataFrame([['Outros', 0], [doc_name, 1]], columns=['label', 'target'])\n",
    "\n",
    "    df = pd.merge(df, df_label, on='label', how='left')\n",
    "\n",
    "    df['target'].fillna(value=0, inplace=True)\n",
    "\n",
    "    df['target'] = df['target'].astype(int)\n",
    "\n",
    "    df_train = df[df['subset'] == 'train']\n",
    "\n",
    "    df_val = df[df['subset'] == 'val']\n",
    "\n",
    "    df_test = df[df['subset'] == 'test']\n",
    "    \n",
    "    X_train, y_train = df_train['texto'].values, df_train['target'].values\n",
    "    X_val, y_val = df_val['texto'].values, df_val['target'].values\n",
    "    X_test, y_test = df_test['texto'].values, df_test['target'].values\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split_dataset(df_data, 'Decisão Inicial')\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVCkvIf-dOgL"
   },
   "source": [
    "## Fine-tunnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1699290453584,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "JHKUWcP1lW2s"
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(tokenizer, X, y, token_size=512, batch_size=4):\n",
    "\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "      list(X),\n",
    "      add_special_tokens=True,\n",
    "      return_attention_mask=True,\n",
    "      pad_to_max_length=True,\n",
    "      max_length=token_size,\n",
    "      return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels = torch.tensor(y)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    dataloader = DataLoader(dataset,\n",
    "                          sampler=RandomSampler(dataset),\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699290453585,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "X8ntIrzMicXF"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_test_total = 0\n",
    "    predictions, true_test = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2]\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_test_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_test.append(label_ids)\n",
    "\n",
    "    loss_test_avg = loss_test_total/len(dataloader)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_test = np.concatenate(true_test, axis=0)\n",
    "\n",
    "    return loss_test_avg, predictions, true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699290453585,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "mlGm5u44i7BU"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def f1_score_func(preds, labels, metric):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=metric)\n",
    "\n",
    "def f1_score_func_average(preds, labels, average_f1):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=average_f1)\n",
    "\n",
    "def accuracy_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return metrics.accuracy_score(labels_flat, preds_flat)\n",
    "\n",
    "def classification_report_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    report = metrics.classification_report(labels_flat,preds_flat)\n",
    "    print(report)\n",
    "\n",
    "def confusion_matrix_class(labels, preds):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return confusion_matrix(labels_flat,preds_flat)\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699290453585,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "2dfNlS-tgNTN"
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader_train, dataloader_val, device='cuda', epochs=10):\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    history = {\n",
    "      'train': {\n",
    "        'f1_weighted': [], 'f1_micro': [], 'f1_macro': [], 'acc': [], 'loss': []\n",
    "      },\n",
    "      'val': {\n",
    "        'f1_weighted': [], 'f1_micro': [], 'f1_macro': [], 'acc': [], 'loss': []\n",
    "      }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_metric = None\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(1, epochs+1)):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm.tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                    }\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "        tqdm.tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "\n",
    "        train_loss, predictions_train, true_train = evaluate(model, dataloader_train, device=device)\n",
    "        val_loss, predictions_val, true_val = evaluate(model, dataloader_val, device=device)\n",
    "\n",
    "        tqdm.tqdm.write(f'Train loss: {train_loss}, Val loss: {val_loss}')\n",
    "\n",
    "        acc_train = accuracy_score_func(predictions_train, true_train)\n",
    "        f1_micro_train = f1_score_func(predictions_train, true_train, 'micro')\n",
    "        f1_macro_train = f1_score_func(predictions_train, true_train, 'macro')\n",
    "        f1_weighted_train = f1_score_func(predictions_train, true_train, 'weighted')\n",
    "\n",
    "        acc_val = accuracy_score_func(predictions_val, true_val)\n",
    "        f1_micro_val = f1_score_func(predictions_val, true_val, 'micro')\n",
    "        f1_macro_val = f1_score_func(predictions_val, true_val, 'macro')\n",
    "        f1_weighted_val = f1_score_func(predictions_val, true_val, 'weighted')\n",
    "\n",
    "        history['train']['acc'].append(acc_train)\n",
    "        history['train']['loss'].append(train_loss)\n",
    "        history['train']['f1_macro'].append(f1_micro_train)\n",
    "        history['train']['f1_micro'].append(f1_macro_train)\n",
    "        history['train']['f1_weighted'].append(f1_weighted_train)\n",
    "\n",
    "        history['val']['acc'].append(acc_val)\n",
    "        history['val']['loss'].append(val_loss)\n",
    "        history['val']['f1_macro'].append(f1_micro_val)\n",
    "        history['val']['f1_micro'].append(f1_macro_val)\n",
    "        history['val']['f1_weighted'].append(f1_weighted_val)\n",
    "\n",
    "        if best_model is None or f1_weighted_val > best_metric:\n",
    "            best_metric = f1_weighted_val\n",
    "            best_model = copy.copy(model)\n",
    "            print(f'best = {epoch} - {np.round(f1_weighted_val, 2)}')\n",
    "\n",
    "    return best_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1699292308238,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "nfa-7OANp_6g"
   },
   "outputs": [],
   "source": [
    "def save_results(history, best_model, doc_name, dataloader_test):\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "\n",
    "    epochs = list(range(len(history['train']['loss'])))\n",
    "\n",
    "    subplots = 1\n",
    "    metrics = ['loss', 'acc', 'f1_macro', 'f1_micro', 'f1_weighted']\n",
    "\n",
    "    for m in metrics:\n",
    "\n",
    "        plt.subplot(3,2,subplots)\n",
    "        plt.plot(epochs, history['train'][m], '.-', label='train')\n",
    "        plt.plot(epochs, history['val'][m], '.-', label='val')\n",
    "        plt.ylabel(m)\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        subplots += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./results/history-{doc_name}.png')\n",
    "    plt.clf()\n",
    "\n",
    "    test_loss, predictions, true_test = evaluate(best_model, dataloader_test, device='cuda')\n",
    "\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(doc_name)\n",
    "    print(classification_report(true_test, preds))\n",
    "    file = open(f'./results/classification-report-{doc_name}.txt', 'w')\n",
    "    file.write(str(classification_report(true_test, preds)))\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "\n",
    "    cm = confusion_matrix_class(true_test, predictions)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis')\n",
    "\n",
    "    plt.xticks([0.5, 1.5], ['Outros', doc_name])\n",
    "    plt.yticks([0.5, 1.5], ['Outros', doc_name])\n",
    "    plt.xlabel('Predito', fontsize=14)\n",
    "    plt.ylabel('Real', fontsize=14)\n",
    "    plt.savefig(f'./results/cm-{doc_name}.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1008846,
     "status": "ok",
     "timestamp": 1699293324614,
     "user": {
      "displayName": "Caio Ponte",
      "userId": "17108623781888323043"
     },
     "user_tz": 180
    },
    "id": "FmMK3soUxh67",
    "outputId": "42c9aee7-e200-419c-8923-b2306712b9c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = ['Despacho ou decisão saneador(a)', 'Decisão Inicial', 'Réplica', 'Petição Inicial']\n",
    "\n",
    "for doc in documents:\n",
    "\n",
    "    print(doc)\n",
    "    print('split train val test')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split_dataset(df_data, doc)\n",
    "\n",
    "    address_model = MODEL\n",
    "    tokenizer = BertTokenizer.from_pretrained(address_model)\n",
    "    model = BertForSequenceClassification.from_pretrained(address_model, num_labels=2, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    print('prepare dataloaders')\n",
    "    dataloader_train = prepare_dataloader(tokenizer, X_train, y_train, token_size=512, batch_size=8)\n",
    "    dataloader_val   = prepare_dataloader(tokenizer, X_val, y_val, token_size=512, batch_size=8)\n",
    "    dataloader_test  = prepare_dataloader(tokenizer, X_test, y_test, token_size=512, batch_size=8)\n",
    "\n",
    "    print('fit model')\n",
    "    best_model, history = fit(model, dataloader_train, dataloader_val, device='cuda', epochs=8)\n",
    "\n",
    "    print('save model')\n",
    "    model_name = address_model.replace('/', '_')\n",
    "    best_model.save_pretrained(f'./results/{model_name}_classbin-{doc}.model')\n",
    "\n",
    "    save_results(history, best_model, doc, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy/SbK9bibdhpRI1medRRX",
   "gpuType": "T4",
   "mount_file_id": "1b9Bz7KRgskgEUcFAYO0hWyKJI6qry18r",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
